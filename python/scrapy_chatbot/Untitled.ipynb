{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Berlin, Lubeck] [visit, stay]\n",
      "User is referring Berlin to visit\n",
      "User is referring Lubeck to stay\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "doc = nlp(u\"What are some places to visit in Berlin and stay in Lubeck\")\n",
    "places = [doc[7], doc[11]] #[Berlin, Lubeck]\n",
    "actions = [doc[5], doc[9]] #[visit, stay]\n",
    "print(places, actions)\n",
    "for place in places:\n",
    "    for tok in place.ancestors:\n",
    "        if tok in actions:\n",
    "            print(\"User is referring {} to {}\".format(place, tok))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Boston Dynamics, thousands, robot dogs]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u\"Boston Dynamics is gearing up to produce thousands of robot dogs\")\n",
    "list(doc.noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep learning learning nsubj cracks\n",
      "the code code dobj cracks\n",
      "messenger RNAs RNAs pobj of\n",
      "potential potential dobj protein-­coding\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"Deep learning cracks the code of messenger RNAs and protein-­coding potential\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "          chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.718274374933058\n",
      "0.3412003733571911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  return _run_code(code, main_globals, None,\n",
      "/usr/lib/python3.8/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  return _run_code(code, main_globals, None,\n"
     ]
    }
   ],
   "source": [
    "hello_doc = nlp(u\"hello\")\n",
    "hi_doc = nlp(u\"hi\")\n",
    "hella_doc = nlp(u\"hella\")\n",
    "print(hello_doc.similarity(hi_doc))\n",
    "print(hello_doc.similarity(hella_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  return _run_code(code, main_globals, None,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6677740235740094"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GoT_str1 = nlp(u\"When will next season of Game of Thrones be releasing?\")\n",
    "GoT_str2 = nlp(u\"Game of Thrones next season release date?\")\n",
    "GoT_str1.similarity(GoT_str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word car is 100% similar to word car\n",
      "Word car is 46% similar to word truck\n",
      "Word car is 23% similar to word google\n",
      "Word truck is 46% similar to word car\n",
      "Word truck is 100% similar to word truck\n",
      "Word truck is 27% similar to word google\n",
      "Word google is 23% similar to word car\n",
      "Word google is 27% similar to word truck\n",
      "Word google is 100% similar to word google\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  return _run_code(code, main_globals, None,\n",
      "/usr/lib/python3.8/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  return _run_code(code, main_globals, None,\n",
      "/usr/lib/python3.8/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  return _run_code(code, main_globals, None,\n",
      "/usr/lib/python3.8/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  return _run_code(code, main_globals, None,\n",
      "/usr/lib/python3.8/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  return _run_code(code, main_globals, None,\n",
      "/usr/lib/python3.8/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  return _run_code(code, main_globals, None,\n"
     ]
    }
   ],
   "source": [
    "example_doc = nlp(u\"car truck google\")\n",
    "for t1 in example_doc:\n",
    "    for t2 in example_doc:\n",
    "        similarity_perc = int(t1.similarity(t2) * 100)\n",
    "        print(\"Word {} is {}% similar to word {}\".format(t1.text, similarity_perc,  t2.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
